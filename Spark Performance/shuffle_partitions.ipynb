{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "pycharm-11ffb9bf",
   "display_name": "PyCharm (MA_Categorization)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Spark Benchmarking\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .load(\"./data/csv/11MB/*.csv\")\n",
    ")\n",
    "small_df = big_df.groupby(\"key\").agg(f.mean(f.col(\"value\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "200"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "small_df.rdd.getNumPartitions() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1000000"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "small_df.persist(pyspark.StorageLevel.DISK_ONLY)\n",
    "small_df.count()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[key: string, avg(value): double]"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "small_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "120000000"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "coalesced = big_df.coalesce(10)\n",
    "coalesced.count()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "120000000"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "repartitioned = big_df.repartition(10)\n",
    "repartitioned.count()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-9ab08477d216>, line 8)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-9ab08477d216>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    small_df = big_df.groupby(\"key\").agg(f.mean(f.col(\"value\")))print(\u001b[0m\n\u001b[0m                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "big_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .load(\"./data/csv/11MB/*.csv\")\n",
    ")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 10)\n",
    "small_df = big_df.groupby(\"key\").agg(f.mean(f.col(\"value\")))print(\n",
    "small_d.count())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .load(\"./data/csv/11MB/*.csv\")\n",
    ")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)\n",
    "\n",
    "small_df = big_df.groupby(\"key\").agg(f.mean(f.col(\"value\")))\n",
    "print(small_df.coalesce(1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .load(\"./data/csv/11MB/*.csv\")\n",
    ")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)\n",
    "\n",
    "small_df = big_df.groupby(\"key\").agg(f.mean(f.col(\"value\")))\n",
    "print(small_df.repartition(1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}